{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_AIML-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYk8NG3yOIT9"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFO6PuxzOIT_"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efNjNImfOIUC",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9C4aAIGOIUH",
        "outputId": "c9c20503-5754-4a0a-cb99-94f2f6c2639c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcoZBStrOIUQ"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYAYf5sB-2gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA1WsFSeOIUS",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnbx7TyQOIUY",
        "colab": {}
      },
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbiHj5YPOIUc",
        "outputId": "cdef4479-bcf7-466d-d58c-02e3b5669ace",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAMQYHfhBXA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37d0614f-cf78-42a0-be6d-d74a41afa590"
      },
      "source": [
        "print(trainY[0:5])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 0 0 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lDAYzkwyOIUj"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBlfYlANOIUk",
        "colab": {}
      },
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RHV3b9mzOIUq",
        "outputId": "d66abe31-d1d3-4bde-fe49-7c52cb306b51",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "First 5 examples now are:  [9 0 0 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2bX5-jZ_i6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwhQ8e7VOIUw"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLkghAkLAB1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5d47a44f-4874-47ec-c392-db4fbd9f3baf"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(trainX[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSmIiUmhKYiJSaEpiIlJoSmIiUmj/H4BqExLuMX2fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvDML2OoOIUx",
        "outputId": "4508341b-88ee-4a1a-a270-73d3c0fb8a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(10):\n",
        "    plt.subplot(1,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(trainX[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[trainY[i]])\n",
        "plt.show()\n",
        "print(\"Label for each of above image:\")\n",
        "print(trainY[0:9])"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAABLCAYAAACfgObJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmG+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label for each of above image:\n",
            "[9 0 0 3 0 2 7 2 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4TbJGeSOIU4"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ac06XZZTOIU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "765095b2-1150-4973-877a-8786c4ecddaa"
      },
      "source": [
        "print(trainX.shape)\n",
        "print(trainY.shape)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Gh54pCGECoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhFXvSN8EcSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hQpLv3aOIU_"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O59C_-IgOIVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "238565b1-4a3f-4c4e-afba-1f2640560db6"
      },
      "source": [
        "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=50)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 2033.2326 - acc: 0.7401 - val_loss: 1638.6974 - val_acc: 0.7637\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1637.5253 - acc: 0.7769 - val_loss: 1781.0311 - val_acc: 0.7446\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1553.4336 - acc: 0.7860 - val_loss: 1929.2948 - val_acc: 0.7651\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1474.4652 - acc: 0.7911 - val_loss: 1686.8015 - val_acc: 0.7346\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1501.1157 - acc: 0.7929 - val_loss: 1461.7347 - val_acc: 0.7981\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1478.4352 - acc: 0.7936 - val_loss: 1078.2669 - val_acc: 0.8005\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1435.1625 - acc: 0.7969 - val_loss: 1931.7350 - val_acc: 0.7408\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1449.2130 - acc: 0.7984 - val_loss: 2841.0876 - val_acc: 0.7076\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1460.9634 - acc: 0.7988 - val_loss: 1280.3210 - val_acc: 0.8022\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1450.3727 - acc: 0.7995 - val_loss: 1332.2263 - val_acc: 0.8040\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1484.5772 - acc: 0.7995 - val_loss: 1377.4304 - val_acc: 0.7941\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1423.0173 - acc: 0.8015 - val_loss: 1034.6405 - val_acc: 0.8145\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1422.5524 - acc: 0.8027 - val_loss: 1282.3955 - val_acc: 0.7962\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1423.4993 - acc: 0.8026 - val_loss: 1456.7113 - val_acc: 0.7789\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1387.6954 - acc: 0.8020 - val_loss: 1058.0913 - val_acc: 0.8038\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1404.8001 - acc: 0.8023 - val_loss: 1164.4013 - val_acc: 0.8106\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1403.5283 - acc: 0.8034 - val_loss: 1796.0986 - val_acc: 0.7712\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1380.2765 - acc: 0.8047 - val_loss: 2055.0776 - val_acc: 0.7509\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1385.0669 - acc: 0.8045 - val_loss: 1067.8300 - val_acc: 0.8172\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1373.8479 - acc: 0.8049 - val_loss: 1878.4927 - val_acc: 0.7446\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1396.3106 - acc: 0.8059 - val_loss: 1067.5543 - val_acc: 0.8252\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1359.7611 - acc: 0.8074 - val_loss: 1547.3930 - val_acc: 0.7738\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 1365.9158 - acc: 0.8063 - val_loss: 3233.7403 - val_acc: 0.7218\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 3s 52us/sample - loss: 1381.8432 - acc: 0.8073 - val_loss: 1062.2965 - val_acc: 0.8230\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 1388.7758 - acc: 0.8059 - val_loss: 1454.7941 - val_acc: 0.7962\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 1364.6020 - acc: 0.8083 - val_loss: 1180.1691 - val_acc: 0.8040\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1364.9815 - acc: 0.8091 - val_loss: 1754.4088 - val_acc: 0.7679\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1370.1227 - acc: 0.8080 - val_loss: 3170.4164 - val_acc: 0.6891\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1385.2343 - acc: 0.8061 - val_loss: 3140.3447 - val_acc: 0.6977\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1385.0582 - acc: 0.8069 - val_loss: 3210.9257 - val_acc: 0.7359\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1382.5565 - acc: 0.8080 - val_loss: 1281.2374 - val_acc: 0.8122\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1378.8201 - acc: 0.8081 - val_loss: 3611.0252 - val_acc: 0.6951\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1392.9469 - acc: 0.8084 - val_loss: 1256.8676 - val_acc: 0.8027\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1361.2019 - acc: 0.8114 - val_loss: 1487.7583 - val_acc: 0.7883\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1389.8836 - acc: 0.8076 - val_loss: 1314.9124 - val_acc: 0.8050\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1353.3277 - acc: 0.8097 - val_loss: 2131.7451 - val_acc: 0.7577\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1343.6289 - acc: 0.8103 - val_loss: 1096.7287 - val_acc: 0.8141\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1331.5451 - acc: 0.8110 - val_loss: 2342.6179 - val_acc: 0.7414\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1374.3454 - acc: 0.8082 - val_loss: 1880.1946 - val_acc: 0.7658\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1369.9857 - acc: 0.8073 - val_loss: 1719.5670 - val_acc: 0.7861\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1346.5376 - acc: 0.8098 - val_loss: 1410.2797 - val_acc: 0.7774\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1358.1438 - acc: 0.8097 - val_loss: 1115.2561 - val_acc: 0.8050\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1375.9920 - acc: 0.8077 - val_loss: 1420.4553 - val_acc: 0.7924\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1358.4216 - acc: 0.8094 - val_loss: 1339.9242 - val_acc: 0.7997\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1350.6404 - acc: 0.8122 - val_loss: 1449.7541 - val_acc: 0.7953\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1382.8874 - acc: 0.8100 - val_loss: 1918.0044 - val_acc: 0.7697\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 1352.3315 - acc: 0.8100 - val_loss: 1486.0548 - val_acc: 0.7911\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1353.0503 - acc: 0.8114 - val_loss: 1801.1040 - val_acc: 0.7598\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 3s 49us/sample - loss: 1328.8101 - acc: 0.8123 - val_loss: 1666.6551 - val_acc: 0.7782\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 1381.6569 - acc: 0.8087 - val_loss: 1281.3520 - val_acc: 0.7971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0488011e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdzDtGwDOIVF"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kndfpdidOIVI",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model1 = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model1.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Normalize the data\n",
        "model1.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model1.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udDapzbjFymM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model1.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwk3T5LJOIVN"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLR8tcBOIVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7850fd85-af32-4018-af08-7047c4000915"
      },
      "source": [
        "model1.fit(trainX, trainY, validation_data=(testX, testY), epochs=50)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6012 - acc: 0.7932 - val_loss: 0.5114 - val_acc: 0.8256\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4914 - acc: 0.8316 - val_loss: 0.4890 - val_acc: 0.8319\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4695 - acc: 0.8377 - val_loss: 0.4952 - val_acc: 0.8342\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4565 - acc: 0.8416 - val_loss: 0.4773 - val_acc: 0.8386\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4487 - acc: 0.8446 - val_loss: 0.4739 - val_acc: 0.8387\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4446 - acc: 0.8465 - val_loss: 0.4678 - val_acc: 0.8415\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4397 - acc: 0.8487 - val_loss: 0.4716 - val_acc: 0.8402\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4335 - acc: 0.8504 - val_loss: 0.4677 - val_acc: 0.8424\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4322 - acc: 0.8501 - val_loss: 0.4638 - val_acc: 0.8408\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4316 - acc: 0.8491 - val_loss: 0.4856 - val_acc: 0.8375\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4282 - acc: 0.8513 - val_loss: 0.4836 - val_acc: 0.8446\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4259 - acc: 0.8506 - val_loss: 0.4906 - val_acc: 0.8409\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4256 - acc: 0.8519 - val_loss: 0.4722 - val_acc: 0.8438\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4252 - acc: 0.8521 - val_loss: 0.4619 - val_acc: 0.8398\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4225 - acc: 0.8528 - val_loss: 0.4662 - val_acc: 0.8424\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4208 - acc: 0.8527 - val_loss: 0.4600 - val_acc: 0.8412\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4195 - acc: 0.8535 - val_loss: 0.4561 - val_acc: 0.8436\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4186 - acc: 0.8535 - val_loss: 0.4633 - val_acc: 0.8429\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4152 - acc: 0.8543 - val_loss: 0.4764 - val_acc: 0.8403\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4169 - acc: 0.8537 - val_loss: 0.4583 - val_acc: 0.8430\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4151 - acc: 0.8558 - val_loss: 0.4675 - val_acc: 0.8412\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4173 - acc: 0.8532 - val_loss: 0.4897 - val_acc: 0.8385\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4145 - acc: 0.8542 - val_loss: 0.4675 - val_acc: 0.8389\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4120 - acc: 0.8549 - val_loss: 0.4675 - val_acc: 0.8359\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4150 - acc: 0.8535 - val_loss: 0.4739 - val_acc: 0.8424\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4130 - acc: 0.8558 - val_loss: 0.4811 - val_acc: 0.8391\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4103 - acc: 0.8564 - val_loss: 0.4824 - val_acc: 0.8399\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4102 - acc: 0.8566 - val_loss: 0.4795 - val_acc: 0.8403\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4125 - acc: 0.8550 - val_loss: 0.4876 - val_acc: 0.8405\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4096 - acc: 0.8565 - val_loss: 0.4791 - val_acc: 0.8374\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4095 - acc: 0.8570 - val_loss: 0.4941 - val_acc: 0.8344\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4085 - acc: 0.8560 - val_loss: 0.4689 - val_acc: 0.8392\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4096 - acc: 0.8555 - val_loss: 0.4725 - val_acc: 0.8409\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4070 - acc: 0.8572 - val_loss: 0.4843 - val_acc: 0.8413\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4060 - acc: 0.8576 - val_loss: 0.4714 - val_acc: 0.8433\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4082 - acc: 0.8560 - val_loss: 0.4750 - val_acc: 0.8399\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4092 - acc: 0.8570 - val_loss: 0.4713 - val_acc: 0.8422\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4086 - acc: 0.8569 - val_loss: 0.4867 - val_acc: 0.8381\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4075 - acc: 0.8566 - val_loss: 0.4710 - val_acc: 0.8380\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4061 - acc: 0.8562 - val_loss: 0.4758 - val_acc: 0.8394\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4072 - acc: 0.8545 - val_loss: 0.4745 - val_acc: 0.8340\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4065 - acc: 0.8576 - val_loss: 0.4713 - val_acc: 0.8427\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4063 - acc: 0.8569 - val_loss: 0.4728 - val_acc: 0.8416\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4069 - acc: 0.8574 - val_loss: 0.5064 - val_acc: 0.8414\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4049 - acc: 0.8572 - val_loss: 0.4764 - val_acc: 0.8386\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4074 - acc: 0.8561 - val_loss: 0.4916 - val_acc: 0.8388\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4053 - acc: 0.8568 - val_loss: 0.4775 - val_acc: 0.8413\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4061 - acc: 0.8569 - val_loss: 0.4687 - val_acc: 0.8429\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.4053 - acc: 0.8562 - val_loss: 0.4864 - val_acc: 0.8403\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4030 - acc: 0.8569 - val_loss: 0.4814 - val_acc: 0.8388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f04881508d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Py-KwkmjOIVU"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLXUE9jWOIVV",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJUqA5T4OIVc",
        "colab": {}
      },
      "source": [
        "learning_rate=0.001\n",
        "sg = SGD(lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcPjDKKyJhHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.compile(optimizer=sg, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V62k7pkfLeKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d48b0487-6d94-459e-daf5-a3cd3b17b831"
      },
      "source": [
        "model1.fit(trainX, trainY, validation_data=(testX, testY), epochs=50)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3956 - acc: 0.8604 - val_loss: 0.4794 - val_acc: 0.8412\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3926 - acc: 0.8613 - val_loss: 0.4622 - val_acc: 0.8423\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3934 - acc: 0.8619 - val_loss: 0.4775 - val_acc: 0.8432\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3933 - acc: 0.8610 - val_loss: 0.4780 - val_acc: 0.8420\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3942 - acc: 0.8615 - val_loss: 0.4686 - val_acc: 0.8436\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3942 - acc: 0.8610 - val_loss: 0.4610 - val_acc: 0.8436\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3927 - acc: 0.8618 - val_loss: 0.4621 - val_acc: 0.8427\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3923 - acc: 0.8616 - val_loss: 0.4797 - val_acc: 0.8423\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 5s 77us/sample - loss: 0.3907 - acc: 0.8624 - val_loss: 0.4650 - val_acc: 0.8438\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3930 - acc: 0.8608 - val_loss: 0.4684 - val_acc: 0.8433\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3921 - acc: 0.8621 - val_loss: 0.4651 - val_acc: 0.8440\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3947 - acc: 0.8605 - val_loss: 0.4609 - val_acc: 0.8428\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3911 - acc: 0.8616 - val_loss: 0.4711 - val_acc: 0.8404\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3917 - acc: 0.8617 - val_loss: 0.4674 - val_acc: 0.8436\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3917 - acc: 0.8622 - val_loss: 0.4644 - val_acc: 0.8412\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3927 - acc: 0.8613 - val_loss: 0.4644 - val_acc: 0.8436\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3918 - acc: 0.8620 - val_loss: 0.4656 - val_acc: 0.8418\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3928 - acc: 0.8601 - val_loss: 0.4640 - val_acc: 0.8428\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3928 - acc: 0.8620 - val_loss: 0.4742 - val_acc: 0.8425\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3919 - acc: 0.8619 - val_loss: 0.4521 - val_acc: 0.8428\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3925 - acc: 0.8611 - val_loss: 0.4580 - val_acc: 0.8429\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3932 - acc: 0.8622 - val_loss: 0.4649 - val_acc: 0.8424\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3901 - acc: 0.8622 - val_loss: 0.4798 - val_acc: 0.8421\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3908 - acc: 0.8627 - val_loss: 0.4786 - val_acc: 0.8423\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3918 - acc: 0.8626 - val_loss: 0.4704 - val_acc: 0.8428\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3926 - acc: 0.8607 - val_loss: 0.4802 - val_acc: 0.8414\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3918 - acc: 0.8608 - val_loss: 0.4574 - val_acc: 0.8430\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3923 - acc: 0.8623 - val_loss: 0.4874 - val_acc: 0.8422\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3914 - acc: 0.8608 - val_loss: 0.4602 - val_acc: 0.8424\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3919 - acc: 0.8618 - val_loss: 0.4588 - val_acc: 0.8420\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3918 - acc: 0.8627 - val_loss: 0.4737 - val_acc: 0.8442\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3916 - acc: 0.8629 - val_loss: 0.4596 - val_acc: 0.8428\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3939 - acc: 0.8619 - val_loss: 0.4649 - val_acc: 0.8429\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3929 - acc: 0.8620 - val_loss: 0.4630 - val_acc: 0.8436\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3924 - acc: 0.8624 - val_loss: 0.4882 - val_acc: 0.8419\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3909 - acc: 0.8627 - val_loss: 0.4605 - val_acc: 0.8422\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3918 - acc: 0.8617 - val_loss: 0.4661 - val_acc: 0.8440\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3904 - acc: 0.8628 - val_loss: 0.4604 - val_acc: 0.8416\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3918 - acc: 0.8613 - val_loss: 0.4642 - val_acc: 0.8428\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3917 - acc: 0.8626 - val_loss: 0.4661 - val_acc: 0.8429\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3916 - acc: 0.8610 - val_loss: 0.4674 - val_acc: 0.8437\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3907 - acc: 0.8618 - val_loss: 0.4740 - val_acc: 0.8427\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3895 - acc: 0.8626 - val_loss: 0.4694 - val_acc: 0.8421\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3914 - acc: 0.8606 - val_loss: 0.4608 - val_acc: 0.8424\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3920 - acc: 0.8615 - val_loss: 0.4782 - val_acc: 0.8435\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3916 - acc: 0.8611 - val_loss: 0.4842 - val_acc: 0.8429\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3916 - acc: 0.8612 - val_loss: 0.4655 - val_acc: 0.8429\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3921 - acc: 0.8623 - val_loss: 0.4697 - val_acc: 0.8432\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3900 - acc: 0.8636 - val_loss: 0.4581 - val_acc: 0.8424\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3924 - acc: 0.8619 - val_loss: 0.4598 - val_acc: 0.8420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f04880b3a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9CSqKvpOIVk"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGAad54JOIVm",
        "colab": {}
      },
      "source": [
        "# Initialize Sequential model\n",
        "model2 = tf.keras.models.Sequential()\n",
        "\n",
        "# Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model2.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "# Normalize the data\n",
        "model2.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "# Add Dense Layer which provides 100 Outputs\n",
        "model2.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "# Add Dense Layer which provides 100 Outputs\n",
        "model2.add(tf.keras.layers.Dense(100, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model2.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr2YsZV0OIV0"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4ojW6-oOIV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d7c519d6-d8bd-4d04-edfb-6046834444dd"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_14 (Reshape)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 92,746\n",
            "Trainable params: 91,178\n",
            "Non-trainable params: 1,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG6kDhrreyvs",
        "colab_type": "text"
      },
      "source": [
        "1) **From the above summary table details of each and every layer is provided.**\n",
        "\n",
        "2) **Total parameters are 92746 among which there are 91178 Trainable parameters and 1568 non_trainable parameters.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfFGmbZLOIV5"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBC8_p7s6_--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e4a953a-8612-4350-ef55-aeb57776a380"
      },
      "source": [
        "learning_rate=0.03\n",
        "sgd = SGD(lr=learning_rate)\n",
        "model2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(trainX, trainY, validation_data=(testX, testY), epochs=50)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 1.0387 - acc: 0.6900 - val_loss: 0.6311 - val_acc: 0.7780\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.5679 - acc: 0.7992 - val_loss: 0.5099 - val_acc: 0.8162\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.4876 - acc: 0.8268 - val_loss: 0.4672 - val_acc: 0.8310\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.4521 - acc: 0.8395 - val_loss: 0.4455 - val_acc: 0.8387\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.4302 - acc: 0.8461 - val_loss: 0.4269 - val_acc: 0.8436\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4127 - acc: 0.8518 - val_loss: 0.4162 - val_acc: 0.8476\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3989 - acc: 0.8576 - val_loss: 0.4049 - val_acc: 0.8515\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3868 - acc: 0.8609 - val_loss: 0.3974 - val_acc: 0.8549\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3775 - acc: 0.8651 - val_loss: 0.3885 - val_acc: 0.8607\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3671 - acc: 0.8691 - val_loss: 0.3797 - val_acc: 0.8606\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.3585 - acc: 0.8714 - val_loss: 0.3763 - val_acc: 0.8634\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.3493 - acc: 0.8741 - val_loss: 0.3709 - val_acc: 0.8638\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3419 - acc: 0.8776 - val_loss: 0.3715 - val_acc: 0.8631\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3360 - acc: 0.8784 - val_loss: 0.3606 - val_acc: 0.8679\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.3308 - acc: 0.8801 - val_loss: 0.3612 - val_acc: 0.8672\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.3238 - acc: 0.8829 - val_loss: 0.3536 - val_acc: 0.8707\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3173 - acc: 0.8861 - val_loss: 0.3530 - val_acc: 0.8710\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3108 - acc: 0.8884 - val_loss: 0.3518 - val_acc: 0.8719\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.3064 - acc: 0.8890 - val_loss: 0.3447 - val_acc: 0.8761\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.3013 - acc: 0.8898 - val_loss: 0.3417 - val_acc: 0.8756\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2958 - acc: 0.8931 - val_loss: 0.3444 - val_acc: 0.8766\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2913 - acc: 0.8933 - val_loss: 0.3380 - val_acc: 0.8774\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2856 - acc: 0.8954 - val_loss: 0.3367 - val_acc: 0.8766\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2825 - acc: 0.8968 - val_loss: 0.3348 - val_acc: 0.8797\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2778 - acc: 0.8994 - val_loss: 0.3344 - val_acc: 0.8778\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2740 - acc: 0.8990 - val_loss: 0.3298 - val_acc: 0.8812\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2694 - acc: 0.9007 - val_loss: 0.3363 - val_acc: 0.8794\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2646 - acc: 0.9033 - val_loss: 0.3327 - val_acc: 0.8793\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2602 - acc: 0.9055 - val_loss: 0.3356 - val_acc: 0.8772\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2586 - acc: 0.9068 - val_loss: 0.3300 - val_acc: 0.8827\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2528 - acc: 0.9077 - val_loss: 0.3335 - val_acc: 0.8763\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2499 - acc: 0.9079 - val_loss: 0.3294 - val_acc: 0.8828\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2467 - acc: 0.9106 - val_loss: 0.3308 - val_acc: 0.8794\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2435 - acc: 0.9107 - val_loss: 0.3331 - val_acc: 0.8797\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2403 - acc: 0.9126 - val_loss: 0.3294 - val_acc: 0.8802\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.2349 - acc: 0.9139 - val_loss: 0.3271 - val_acc: 0.8840\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2326 - acc: 0.9147 - val_loss: 0.3267 - val_acc: 0.8823\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2285 - acc: 0.9175 - val_loss: 0.3295 - val_acc: 0.8813\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2270 - acc: 0.9172 - val_loss: 0.3277 - val_acc: 0.8803\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2237 - acc: 0.9187 - val_loss: 0.3238 - val_acc: 0.8822\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.2201 - acc: 0.9197 - val_loss: 0.3232 - val_acc: 0.8848\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2180 - acc: 0.9203 - val_loss: 0.3302 - val_acc: 0.8820\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.2154 - acc: 0.9215 - val_loss: 0.3271 - val_acc: 0.8850\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2136 - acc: 0.9212 - val_loss: 0.3387 - val_acc: 0.8819\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2080 - acc: 0.9240 - val_loss: 0.3266 - val_acc: 0.8834\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 6s 103us/sample - loss: 0.2074 - acc: 0.9238 - val_loss: 0.3360 - val_acc: 0.8788\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.2042 - acc: 0.9259 - val_loss: 0.3257 - val_acc: 0.8822\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.2005 - acc: 0.9281 - val_loss: 0.3398 - val_acc: 0.8815\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.1991 - acc: 0.9273 - val_loss: 0.3393 - val_acc: 0.8804\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.1956 - acc: 0.9279 - val_loss: 0.3358 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0487ef3710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ_SLyyVfY0l",
        "colab_type": "text"
      },
      "source": [
        "**1) When only sgd optimizer was used accuracy was less and loss was higher compared with all models**\n",
        "\n",
        "**2) When Batch Normalization layer was added, accuracy improved and loss became less**\n",
        "\n",
        "**3) The accuracy further improved and loss become some more minimal as learning rate was added to the layers.**\n",
        "\n",
        "**4) When more hidden layers with 100 neurons per layer is added and learning rate is also increased, with batch normalization, finally the test accuracy was at 88% with loss being minimal at 34%**\n"
      ]
    }
  ]
}